{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/r3dmaohong/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: Darwin-16.6.0-x86_64-i386-64bit\n",
      "Tensorflow version: 1.4.1\n",
      "Keras version: 2.1.2\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import tensorflow\n",
    "import keras\n",
    "print(\"Platform: {}\".format(platform.platform()))\n",
    "print(\"Tensorflow version: {}\".format(tensorflow.__version__))\n",
    "print(\"Keras version: {}\".format(keras.__version__))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 376s 1us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "model_vgg16 = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "url = \"http://assets.worldwildlife.org/photos/11048/images/story_full_width/rsz_namibia_will_burrard_lucas_wwf_us_2.jpg?1461768823\"\n",
    "from urllib import request\n",
    "request.urlretrieve(url, \"elephant.jpg\")\n",
    "\n",
    "img_file = 'elephant.jpg'\n",
    "image = load_img(img_file, target_size=(224, 224)) # input size of vgg16 is 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape: (224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "# to numpy\n",
    "image = img_to_array(image) \n",
    "print(\"image.shape:\", image.shape) #rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape: (1, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "print(\"image.shape:\", image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "image = preprocess_input(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_vgg16.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n",
      "40960/35363 [==================================] - 1s 23us/step\n",
      "African_elephant (56.16%)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import decode_predictions\n",
    "\n",
    "# turn probability to labels\n",
    "label = decode_predictions(y_pred)\n",
    "# highest probability\n",
    "label = label[0][0]\n",
    "\n",
    "print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n",
      "574717952/574710816 [==============================] - 467s 1us/step\n",
      "image.shape: (224, 224, 3)\n",
      "image.shape: (1, 224, 224, 3)\n",
      "tusker (52.69%)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.applications.vgg19 import decode_predictions\n",
    "\n",
    "\n",
    "model_vgg19 = VGG19(weights='imagenet')\n",
    "image = load_img(img_file, target_size=(224, 224)) # 因為VGG19的模型的輸入是224x224\n",
    "\n",
    "image = img_to_array(image) # RGB\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = preprocess_input(image)\n",
    "y_pred = model_vgg19.predict(image)\n",
    "label = decode_predictions(y_pred)\n",
    "label = label[0][0]\n",
    "\n",
    "print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "102858752/102853048 [==============================] - 194s 2us/step\n",
      "image.shape: (224, 224, 3)\n",
      "image.shape: (1, 224, 224, 3)\n",
      "African_elephant (30.52%)\n"
     ]
    }
   ],
   "source": [
    "# resnet50\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.applications.resnet50 import decode_predictions\n",
    "\n",
    "model_resnet50 = ResNet50(weights='imagenet')\n",
    "\n",
    "image = load_img(img_file, target_size=(224, 224)) \n",
    "\n",
    "image = img_to_array(image) # RGB\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = preprocess_input(image)\n",
    "y_pred = model_resnet50.predict(image)\n",
    "label = decode_predictions(y_pred)\n",
    "\n",
    "label = label[0][0]\n",
    "\n",
    "print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape: (299, 299, 3)\n",
      "image.shape: (1, 299, 299, 3)\n",
      "African_elephant (52.15%)\n"
     ]
    }
   ],
   "source": [
    "# InceptionV3\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.applications.inception_v3 import decode_predictions\n",
    "\n",
    "model_inception_v3 = InceptionV3(weights='imagenet')\n",
    "\n",
    "img = load_img(img_file, target_size=(299, 299)) \n",
    "\n",
    "image = image.img_to_array(img) # RGB\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = preprocess_input(image)\n",
    "\n",
    "y_pred = model_inception_v3.predict(image)\n",
    "label = decode_predictions(y_pred)\n",
    "label = label[0][0]\n",
    "\n",
    "print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5\n",
      "225214464/225209952 [==============================] - 163s 1us/step\n",
      "image.shape: (299, 299, 3)\n",
      "image.shape: (1, 299, 299, 3)\n",
      "African_elephant (66.42%)\n"
     ]
    }
   ],
   "source": [
    "# InceptionResNetV2\n",
    "\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.applications.inception_resnet_v2 import decode_predictions\n",
    "\n",
    "model_inception_resnet_v2 = InceptionResNetV2(weights='imagenet')\n",
    "\n",
    "image = load_img(img_file, target_size=(299, 299)) \n",
    "\n",
    "image = img_to_array(image) # RGB\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = preprocess_input(image)\n",
    "\n",
    "y_pred = model_inception_resnet_v2.predict(image)\n",
    "label = decode_predictions(y_pred)\n",
    "label = label[0][0]\n",
    "\n",
    "print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf.h5\n",
      "17227776/17225924 [==============================] - 9s 1us/step\n",
      "image.shape: (224, 224, 3)\n",
      "image.shape: (1, 224, 224, 3)\n",
      "tusker (60.01%)\n"
     ]
    }
   ],
   "source": [
    "# MobileNet\n",
    "\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.applications.mobilenet import decode_predictions\n",
    "\n",
    "model_mobilenet = MobileNet(weights='imagenet')\n",
    "\n",
    "image = load_img(img_file, target_size=(224, 224)) \n",
    "\n",
    "image = img_to_array(image) # RGB\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = preprocess_input(image)\n",
    "\n",
    "y_pred = model_mobilenet.predict(image)\n",
    "\n",
    "label = decode_predictions(y_pred)\n",
    "\n",
    "label = label[0][0]\n",
    "\n",
    "print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
