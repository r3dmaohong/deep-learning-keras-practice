{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from skimage import io, color, exposure, transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import glob\n",
    "import h5py #It lets you store huge amounts of numerical data, and easily manipulate that data from NumPy. \n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "import keras.utils as np_utils\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 43\n",
    "IMG_SIZE = 48\n",
    "\n",
    "wp_dir = 'data/traffic-sign-recognition/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import PurePath\n",
    "\n",
    "def preprocess_img(img):\n",
    "    # Histogram equalization\n",
    "    hsv = color.rgb2hsv(img)\n",
    "    hsv[:,:,2] = exposure.equalize_hist(hsv[:,:,2])\n",
    "    img = color.hsv2rgb(hsv)\n",
    "    \n",
    "    # center\n",
    "    min_side = min(img.shape[:-1])\n",
    "    centre = img.shape[0]//2, img.shape[1]//2\n",
    "    img = img[centre[0]-min_side//2:centre[0]+min_side//2,\n",
    "              centre[1]-min_side//2:centre[1]+min_side//2,\n",
    "              :]\n",
    "    \n",
    "    # size\n",
    "    img = transform.resize(img, (IMG_SIZE, IMG_SIZE))    \n",
    "    return img\n",
    "\n",
    "# get label\n",
    "def get_class(img_path):\n",
    "    return int(PurePath(img_path).parts[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to import data from local. If the file don't exist, process all the images.\n",
    "try:\n",
    "    with h5py.File('X.h5') as hf:\n",
    "        X, Y = hf['imgs'][:], hf['labels'][:]\n",
    "    \n",
    "    print(\"Loaded images from X.h5\")\n",
    "    \n",
    "except(IOError, OSError, KeyError):\n",
    "    print(\"Error in reading X.h5. Processing all images...\")\n",
    "    root_dir = wp_dir + 'GTSRB/Final_Training/Images/'\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    \n",
    "    all_img_paths = glob.glob(os.path.join(root_dir, '*/*.ppm'))\n",
    "    np.random.shuffle(all_img_paths)\n",
    "    for img_path in all_img_paths:\n",
    "        try:\n",
    "            img = preprocess_img(io.imread(img_path))\n",
    "            label = get_class(img_path)\n",
    "            imgs.append(img)\n",
    "            labels.append(label)\n",
    "            \n",
    "            if len(imgs)%5000 == 0:\n",
    "                print(\"Processed {}/{}\".format(len(imgs), len(all_img_paths)))\n",
    "        except(IOError, OSError):\n",
    "            print('missed', img_path)\n",
    "            pass\n",
    "        \n",
    "    X = np.array(imgs, dtype='float32')\n",
    "    # one hot encoding\n",
    "    Y = np.eye(NUM_CLASSES, dtype='uint8')[labels]\n",
    "    \n",
    "    # save to local\n",
    "    with h5py.File('X.h5', 'w') as hf:\n",
    "        hf.create_dataset('imgs', data=X)\n",
    "        hf.create_dataset('labels', data=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)))    \n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    \n",
    "    return model;   \n",
    "\n",
    "model = cnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=sgd,\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    return lr*(0.1**int(epoch/10))\n",
    "\n",
    "batch_size = 32\n",
    "nb_epoch = 30\n",
    "\n",
    "history = model.fit(X, Y,\n",
    "         batch_size=batch_size,\n",
    "         epochs=nb_epoch,\n",
    "         validation_split=0.2,\n",
    "         shuffle=True,\n",
    "         callbacks=[LearningRateScheduler(lr_schedule),\n",
    "             ModelCheckpoint('model.h5', save_best_only=True)\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_train_history(history, train_metrics, val_metrics):\n",
    "    plt.plot(history.history.get(train_metrics),'-o')\n",
    "    plt.plot(history.history.get(val_metrics),'-o')\n",
    "    plt.ylabel(train_metrics)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(['train', 'validation'])\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plot_train_history(history, 'loss','val_loss')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plot_train_history(history, 'acc','val_acc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Filename;Width;Height;Roi.X1;Roi.Y1;Roi.X2;Roi.Y2;ClassId\n",
    "test = pd.read_csv(wp_dir + 'GTSRB/GT-final_test.csv', sep=';') \n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "i=0\n",
    "for file_name, class_id in zip(list(test['Filename']),list(test['ClassId'])):\n",
    "    img_path = os.path.join(wp_dir, 'GTSRB/Final_Test/Images/',file_name)\n",
    "    X_test.append(preprocess_img(io.imread(img_path)))\n",
    "    y_test.append(class_id)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(\"X_test.shape: \", X_test.shape)\n",
    "print(\"y_test.shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test)\n",
    "acc = np.sum(y_pred==y_test)/np.size(y_pred)\n",
    "print(\"Test accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=False, \n",
    "                            featurewise_std_normalization=False, \n",
    "                            rotation_range=10.,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,                             \n",
    "                            shear_range=0.1,\n",
    "                            zoom_range=0.2,\n",
    "                            )\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_model()\n",
    "\n",
    "lr = 0.01\n",
    "sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=sgd,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    return lr*(0.1**int(epoch/10))\n",
    "\n",
    "batch_size = 32\n",
    "nb_epoch = 30\n",
    "\n",
    "history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "                            steps_per_epoch=X_train.shape[0]/batch_size,\n",
    "                            epochs=nb_epoch,\n",
    "                            validation_data=(X_val, Y_val),\n",
    "                            callbacks=[LearningRateScheduler(lr_schedule),\n",
    "                                       ModelCheckpoint('model2.h5',save_best_only=True)]\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_train_history(history, train_metrics, val_metrics):\n",
    "    plt.plot(history.history.get(train_metrics),'-o')\n",
    "    plt.plot(history.history.get(val_metrics),'-o')\n",
    "    plt.ylabel(train_metrics)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(['train', 'validation'])\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plot_train_history(history, 'loss','val_loss')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plot_train_history(history, 'acc','val_acc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test)\n",
    "acc = np.sum(y_pred==y_test)/np.size(y_pred)\n",
    "print(\"Test accuracy = {}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
